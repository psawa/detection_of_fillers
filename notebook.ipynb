{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the audios\n",
    "system(\"wget --accept wav --mirror --no-parent https://media.talkbank.org/ca/CallHome/eng/0wav/ -P ./corpus/original_audio/\")\n",
    "for filename in glob.iglob('./corpus/original_audio/media.talkbank.org/ca/CallHome/eng/0wav/*'):\n",
    "    system('mv {} ./corpus/original/audio')\n",
    "system('rm -r ./corpus/original_audio/media.talkbank.org')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the transcripts\n",
    "system('wget https://ca.talkbank.org/data/CallHome/eng.zip -P ./corpus/original_text/')\n",
    "system('unzip -j ./corpus/original_text/eng.zip -d ./corpus/original_text/')\n",
    "for filename in glob.iglob('./corpus/original_text/*'):\n",
    "    if not filename.endswith('.cha'):\n",
    "        system('rm {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence level alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk, listdir\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speechs = {}\n",
    "\n",
    "class Speech():\n",
    "    def __init__(self, identifier):\n",
    "        self._id = identifier\n",
    "        self.audio_path = './corpus/original_audio/'+identifier+'.wav'\n",
    "        self.transc_path = './corpus/original_text/'+identifier+'.cha'\n",
    "        self.sentences = [] #dictionnaries like {start (ms), end, text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instances\n",
    "for filename in listdir('./corpus/original_audio/'):\n",
    "        if filename.endswith('.wav'):\n",
    "            _id = filename.split('.')[0]\n",
    "            speechs[_id] = Speech(_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_lines(content):\n",
    "        try:\n",
    "            content = re.split('@Media.*audio\\n',content)[1]\n",
    "        except:\n",
    "            return \n",
    "        content = re.split('\\n@End\\n',content)[0]\n",
    "        lines = re.split('(\\x15.*\\x15)', content)\n",
    "        lines2 = []\n",
    "        i = 0\n",
    "        pattern = re.compile('\\x15.*\\x15')\n",
    "        while i<len(lines):\n",
    "            elem = lines[i]\n",
    "            while not pattern.match(lines[i]) and i!=len(lines)-1: #tant ne contient pas (ou n'est pas) un timestamp:\n",
    "                i += 1\n",
    "                next_elem = lines[i]\n",
    "                next_elem = next_elem.replace('*B:', ' ')\n",
    "                next_elem = next_elem.replace('*A:', ' ')\n",
    "                elem = elem + ' ' + next_elem #on concatenate avec l'Ã©lement suivant\n",
    "            #quand on tombe sur un timestamp\n",
    "            elem = elem.replace('\\n\\t', ' ')\n",
    "            elem = elem.replace('\\n', '')\n",
    "            elem = elem.replace('\\t','')\n",
    "            elem = elem.replace('*A:',' ')\n",
    "            elem = elem.replace('*B:',' ')\n",
    "            elem= elem[1:]\n",
    "            \n",
    "            lines2.append(elem)\n",
    "            i += 1 \n",
    "    \n",
    "        lines2 = lines2[:-1]\n",
    "        return lines2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotting the sentences\n",
    "for speech in speechs.values():\n",
    "        with open(speech.transc_path) as transc_file:\n",
    "            content = transc_file.read()\n",
    "            \n",
    "        #splitting after each timestamp\n",
    "        lines= get_clean_lines(content)\n",
    "        if lines == None:\n",
    "            continue\n",
    "        \n",
    "        for i in range(len(lines)):\n",
    "            lines[i] = lines[i].rstrip()\n",
    "        for line in lines:\n",
    "            try:\n",
    "                timestamp = re.search(\"\\x15(.*)\\x15\", line).group(1)\n",
    "            except:\n",
    "                breakpoint()\n",
    "            start = int(timestamp.split(\"_\")[0])\n",
    "            end = int(timestamp.split(\"_\")[1])\n",
    "            #breakpoint()\n",
    "            text = re.search(\"(.*)\\x15{}\".format(timestamp),line).group(1)\n",
    "            #writing in the speech object\n",
    "            speech.sentences.append({'start':start,'end':end, 'text':text})\n",
    "print(\"Sentences retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating files\n",
    "for speech in speechs.values():\n",
    "        print('Cutting {}'.format(speech.audio_path))\n",
    "        data, samplerate = sf.read(speech.audio_path)\n",
    "        for sent in speech.sentences:\n",
    "            title = \"{}_{}_{}\".format(speech._id, sent['start'], sent['end'])\n",
    "            cut = data[sent['start']*int((samplerate/1000)):sent['end']*int((samplerate/1000))]\n",
    "            sf.write('./corpus/alignment/sentence_level_audio/{}.wav'.format(title),cut , samplerate)\n",
    "            with open('./corpus/alignment/sentence_level_text/{}.txt'.format(title),'w') as outfile:\n",
    "                print(sent['text'], file = outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering non-filler utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('(mhm|uhhuh|mm|um|eh|em|ah|huh|ha|er|oof|hee|ach|eee|ew)')\n",
    "\n",
    "for file in glob.iglob('./corpus/alignment_sentence_level_text/'):\n",
    "    filename = file.split('.')[-1]\n",
    "    filename = filename.split('.')[0]\n",
    "    has_filler = False\n",
    "    with open(file,'r', encoding = 'utf-8') as infile:\n",
    "        content = infile.read()\n",
    "        matched = re.match(pattern, content)\n",
    "        has_filler = bool(matched)\n",
    "    if not has_filler:\n",
    "        system('rm {}'.format(file))\n",
    "        system('rm ./corpus/alignment_sentence_level_audio/{}.wav'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word level alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the audio sentences into mono\n",
    "path = './corpus/alignment/sentence_level_audio/'\n",
    "for filename in listdir(path):\n",
    "    system('sox {}{} {}{} remix 1,2'.format(path, filename, path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the setup for MFA\n",
    "# (converting .txt to .lab, and moving .lab and .wav sentence level into the same folder)\n",
    "\n",
    "for filename in listdir('./corpus/alignment/sentence_level_audio'):\n",
    "    _id = filename.split('.')[0]\n",
    "    system('mv ./corpus/alignment/sentence_level_audio/{}.wav ./corpus/alignment/mfa_setup/{}.wav '.format(_id,_id))\n",
    "    system('mv ./corpus/alignment/sentence_level_text/{}.txt ./corpus/alignment/mfa_setup/{}.lab'.format(_id,_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfa_path = ??? # Replace with the installation directory of Montreal Forced Aligner\n",
    "lm_path = '../corpus/alignment/librispeech-lexicon.txt' # Language model\n",
    "lab_wav_path = '../corpus/alignment/mfa_setup'\n",
    "output_path = '../corpus/alignment/textgrids'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfa command\n",
    "system('{}\\bin\\mfa_align.exe -v  {} {} english {}'.format(mfa_path, lm_path, lab_wav_path, output_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrids as tg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hesitations = [\n",
    "    'mhm',\n",
    "    'uhhuh',\n",
    "    'mm',\n",
    "    'um',\n",
    "    'eh',\n",
    "    'em',\n",
    "    'ah',\n",
    "    'huh',\n",
    "    'ha',\n",
    "    'er',\n",
    "    'oof',\n",
    "    'hee',\n",
    "    'ach',\n",
    "    'eee',\n",
    "    'ew'\n",
    "]\n",
    "\n",
    "silence = ['sp', 'sil','']\n",
    "\n",
    "def tag_word(word):\n",
    "    if word in hesitations: \n",
    "        return 'hesitation'\n",
    "    elif word in silence:\n",
    "        return 'silence'\n",
    "    else:\n",
    "        return 'speech'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can jump until the end if you have the pickles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(columns=['file','word','tag','xmin','xmax'])\n",
    "files_df = pd.DataFrame(columns=['file', 'xmin','xmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in listdir('./corpus/alignment/textgrids'):\n",
    "    if not filename.endswith('.TextGrid'):\n",
    "        continue\n",
    "    name = filename.split('.')[0]\n",
    "    the_tg = tg.TextGrid('./corpus/alignment/textgrids/'+filename)\n",
    "    \n",
    "    # Filling the dataframe of files\n",
    "    files_df.loc[len(files_df)] = {'file':name,\n",
    "                            'xmin':the_tg.xmin,\n",
    "                            'xmax':the_tg.xmax}\n",
    "    \n",
    "    # Filling the dataframe of words \n",
    "    for word in the_tg['words']:\n",
    "        words_df.loc[len(words_df)] = {'file':name,\n",
    "                             'word':word.text,\n",
    "                             'tag':tag_word(word.text),\n",
    "                             'xmin':word.xmin,\n",
    "                             'xmax':word.xmax\n",
    "                            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df['duration'] = words_df['xmax']-words_df['xmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df['duration'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to ms\n",
    "words_df['xmin'] = words_df['xmin'].apply(lambda x: int(x*1000))\n",
    "words_df['xmax'] = words_df['xmax'].apply(lambda x: int(x*1000))\n",
    "words_df['duration'] = words_df['duration'].apply(lambda x: int(x*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.groupby('tag').count().file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df['duration'] = files_df['xmax']-files_df['xmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to ms\n",
    "files_df['xmin'] = files_df['xmin'].apply(lambda x: int(x*1000))\n",
    "files_df['xmax'] = files_df['xmax'].apply(lambda x: int(x*1000))\n",
    "files_df['duration'] = files_df['duration'].apply(lambda x: int(x*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.to_pickle('./pickles/words_df.pkl')\n",
    "files_df.to_pickle('./pickles/files_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.read_pickle('./pickles/words_df.pkl')\n",
    "files_df = pd.read_pickle('./pickles/files_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_decision_med(filename, xmin, xmax):\n",
    "    # Taking the median sample\n",
    "    med = int((xmin+xmax)/2)\n",
    "    query = words_df.query(\"file=='{}' and {} >= xmin and {} <= xmax \".format(filename, med, med))\n",
    "    tag = query.iloc[0]['tag']\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2int = {\n",
    "    '<dummy>':0,\n",
    "    'speech':1,\n",
    "    'silence':2,\n",
    "    'hesitation':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "win_shift = 12.5\n",
    "win_len = 12.5\n",
    "numb_files = len(files_df)\n",
    "max_len = int(files_df['duration'].max() // win_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip the following if you have tags.pt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.zeros((numb_files,max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = 0,0\n",
    "\n",
    "for audiofile in files_df['file']:\n",
    "    j = 0\n",
    "    filename = audiofile\n",
    "    file_df = files_df.query(\"file == '{}'\".format(filename))\n",
    "    duration = file_df.iloc[0]['duration']\n",
    "    head = 0\n",
    "    while head + win_shift <= duration:\n",
    "        try:\n",
    "            tag = tag2int[tag_decision_med(filename, head, head+win_len )]\n",
    "            Y[i][j] = tag\n",
    "        except:\n",
    "            continue\n",
    "        head += win_shift\n",
    "        j += 1\n",
    "    i += 1\n",
    "    print('{} / {}'.format(i,numb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(Y, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tags,'./pickles/Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.load('./pickles/Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_tensors = []\n",
    "\n",
    "for audiofile in files_df['file']:\n",
    "    waveform, sample_rate = torchaudio.load('./corpus/alignment/mfa_setup/'+audiofile+'.wav')\n",
    "    mfcc = torchaudio.transforms.MFCC(n_mfcc=13)(waveform)\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    mfcc_tensors.append(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tensors = []\n",
    "\n",
    "for tensor in mfcc_tensors:\n",
    "    target = torch.zeros(1, 13, max_len+1)\n",
    "    source = tensor\n",
    "    target[:, :, :tensor.shape[2]] = source\n",
    "    padded_tensors.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat((padded_tensors), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.narrow(X, 2, 0, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X,'./pickles/X.pt')\n",
    "torch.save(Y,'./pickles/Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load('./pickles/X.pt')\n",
    "Y = torch.load('./pickles/Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load('./pickles/X.pt')\n",
    "Y = torch.load('./pickles/Y.pt').squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_STEPS = X.shape[2]\n",
    "N_INPUTS = 13\n",
    "N_NEURONS = 150\n",
    "N_OUTPUTS = 4\n",
    "N_EPHOCS = 50\n",
    "LR = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "train_set = TensorDataset(X_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, Y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, batch_size, n_steps, n_inputs, n_neurons, n_outputs):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_neurons = n_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps = n_steps\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons)\n",
    "        \n",
    "        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n",
    "        \n",
    "    def init_hidden(self,):\n",
    "        # (num_layers, batch_size, n_neurons)\n",
    "        return (torch.zeros(1, self.batch_size, self.n_neurons))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # transforms X to dimensions: n_steps X batch_size X n_inputs\n",
    "        X = X.permute(1, 0, 2)\n",
    "\n",
    "        self.batch_size = X.size(1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        lstm_out, self.hidden = self.basic_rnn(X, self.hidden)\n",
    "        out = self.FC(lstm_out)\n",
    "\n",
    "        return out.view(-1, self.n_outputs) # batch_size X n_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model instance\n",
    "model = RNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS)\n",
    "criterion = nn.CrossEntropyLoss(size_average=True, ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def get_accuracy(logit, target, batch_size):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100 * corrects/batch_size\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPHOCS):  # loop over the dataset multiple times\n",
    "    train_running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    outs = []\n",
    "    golds = []\n",
    "    model.train()\n",
    "    \n",
    "    # TRAINING ROUND\n",
    "    for i, data in enumerate(train_loader):\n",
    "         # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # reset hidden states\n",
    "        model.hidden = model.init_hidden() \n",
    "        \n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, N_STEPS, N_INPUTS)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels.long().view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.detach().item()\n",
    "        train_acc += get_accuracy(outputs, labels.long().view(-1), BATCH_SIZE)\n",
    "        \n",
    "        out = torch.max(outputs, 1)[1].view(labels.long().view(-1).size()).data\n",
    "        gold = labels.long().view(-1).data\n",
    "        outs.append(out)\n",
    "        golds.append(gold)\n",
    "         \n",
    "    model.eval()\n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' \n",
    "          %(epoch+1, train_running_loss/i, train_acc/(i*N_STEPS)))\n",
    "    \n",
    "    y_pred = torch.cat(outs)\n",
    "    y_gold = torch.cat(golds)\n",
    "\n",
    "    print(confusion_matrix(y_gold, y_pred, labels=[1, 2, 3]))\n",
    "    print(classification_report(y_gold, y_pred, labels=[1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = 0.0\n",
    "outs = []\n",
    "golds = []\n",
    "\n",
    "for i, data in enumerate(valid_loader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.view(-1, N_STEPS, N_INPUTS)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    test_acc += get_accuracy(outputs, labels.long().view(-1), BATCH_SIZE)\n",
    "    out = torch.max(outputs, 1)[1].view(labels.long().view(-1).size()).data\n",
    "    gold = labels.long().view(-1).data\n",
    "    outs.append(out)\n",
    "    golds.append(gold)\n",
    "        \n",
    "print('Test Accuracy: %.2f'%(test_acc/(i*N_STEPS)))\n",
    "\n",
    "y_pred = torch.cat(outs)\n",
    "y_gold = torch.cat(golds)\n",
    "\n",
    "print(confusion_matrix(y_gold, y_pred, labels=[1, 2, 3]))\n",
    "print(classification_report(y_gold, y_pred, labels=[1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = pd.DataFrame({'speech':[307140,673,894],'silence':[67293,146,147],'filler':[77599,198,292]}, index=[\"speech\", \"silence\", \"filler\"])\n",
    "matrix = sns.heatmap(cm1.T, annot=True, fmt='d', linewidths=.5, cmap='coolwarm')\n",
    "matrix.set(xlabel='predicted label', ylabel='true label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame({'speech':[38602,0,0],'silence':[8888,0,0],'filler':[11674,0,0]}, index=[\"speech\", \"silence\", \"filler\"])\n",
    "matrix = sns.heatmap(cm.T, annot=True, fmt='d', linewidths=.5, cmap='coolwarm')\n",
    "matrix.set(xlabel='predicted label', ylabel='true label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
